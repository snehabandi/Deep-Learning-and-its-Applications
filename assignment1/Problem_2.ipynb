{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Incorporating CNNs\n",
    "\n",
    "* Learning Objective: In this problem, you will learn how to deeply understand how Convolutional Neural Networks work by implementing one.\n",
    "* Provided Code: We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* TODOs: you will implement a Convolutional Layer and a MaxPooling Layer to improve on your classification results in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from lib.mlp.fully_conn import *\n",
    "from lib.mlp.layer_utils import *\n",
    "from lib.mlp.train import *\n",
    "from lib.cnn.layer_utils import *\n",
    "from lib.cnn.cnn_models import *\n",
    "from lib.datasets import *\n",
    "from lib.grad_check import *\n",
    "from lib.optim import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (CIFAR-100 with 20 superclasses)\n",
    "\n",
    "In this homework, we will be classifying images from the CIFAR-100 dataset into the 20 superclasses. More information about the CIFAR-100 dataset and the 20 superclasses can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Download the CIFAR-100 data files [here](https://drive.google.com/drive/folders/1imXxTnpkMbWEe41pkAGNt_JMTXECDSaW?usp=share_link), and save the `.mat` files to the `data/cifar100` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: data_train Shape: (40000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_train Shape: (40000,), <class 'numpy.ndarray'>\n",
      "Name: data_val Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_val Shape: (10000,), <class 'numpy.ndarray'>\n",
      "Name: data_test Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_test Shape: (10000,), <class 'numpy.ndarray'>\n",
      "label_names: ['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2']\n",
      "Name: mean_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n",
      "Name: std_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR100_data('data/cifar100/')\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print (\"Name: {} Shape: {}, {}\".format(k, v.shape, type(v)))\n",
    "    else:\n",
    "        print(\"{}: {}\".format(k, v))\n",
    "label_names = data['label_names']\n",
    "mean_image = data['mean_image'][0]\n",
    "std_image = data['std_image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: large_omnivores_and_herbivores\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "image_data = data['data_train'][idx]\n",
    "image_data = ((image_data*std_image + mean_image) * 255).astype(np.int32)\n",
    "plt.imshow(image_data)\n",
    "label = label_names[data['labels_train'][idx]]\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "We will use convolutional neural networks to try to improve on the results from Problem 1. Convolutional layers make the assumption that local pixels are more important for prediction than far-away pixels. This allows us to form networks that are robust to small changes in positioning in images.\n",
    "\n",
    "### Convolutional Layer Output size calculation [2pts]\n",
    "\n",
    "As you have learned, two important parameters of a convolutional layer are its stride and padding. To warm up, we will need to calculate the output size of a convolutional layer given its stride and padding. To do this, open the `lib/cnn/layer_utils.py` file and fill out the TODO section in the `get_output_size` function in the ConvLayer2D class. \n",
    "\n",
    "Implement your function so that it returns the correct size as indicated by the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received [32, 16, 16, 16] and expected [32, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "input_image = np.zeros([32, 28, 28, 3]) # a stack of 32 28 by 28 rgb images\n",
    "\n",
    "in_channels = input_image.shape[-1] #must agree with the last dimension of the input image\n",
    "k_size = 4 \n",
    "n_filt = 16\n",
    "\n",
    "conv_layer = ConvLayer2D(in_channels, k_size, n_filt, stride=2, padding=3)\n",
    "output_size = conv_layer.get_output_size(input_image.shape) \n",
    "\n",
    "print(\"Received {} and expected [32, 16, 16, 16]\".format(output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "32 28 28 3\n"
     ]
    }
   ],
   "source": [
    "#REMOVE LATER\n",
    "input_image_shape = [32,28,28,3] # (batch_size, in_height, in_width, in_channels)\n",
    "in_height = input_image_shape[1]\n",
    "in_width = input_image_shape[2]\n",
    "in_channel = 3 # RGB\n",
    "\n",
    "k_size # kernel size\n",
    "n_filt # Filter size\n",
    "stride = 2 # Stride\n",
    "padding = 3 # Padding\n",
    "\n",
    "#((i-k+2p)/s)  + 1\n",
    "dim = ((in_height-k_size+(2*padding))//stride) + 1\n",
    "print(dim)\n",
    "\n",
    "batch_size, in_height, in_width, in_channels = input_image.shape\n",
    "print(batch_size, in_height, in_width, in_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer Forward Pass [5pts]\n",
    "\n",
    "Now, we will implement the forward pass of a convolutional layer. Fill in the TODO block in the `forward` function of the ConvLayer2D class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received output shape: (1, 4, 4, 2), Expected output shape: (1, 4, 4, 2)\n",
      "Difference:  5.110565335399418e-08\n",
      "[[[[-0.03874312  0.57000324]\n",
      "   [-0.03955296  0.57081309]\n",
      "   [-0.04036281  0.57162293]\n",
      "   [-0.04117266  0.57243278]]\n",
      "\n",
      "  [[-0.0452219   0.57648202]\n",
      "   [-0.04603175  0.57729187]\n",
      "   [-0.04684159  0.57810172]\n",
      "   [-0.04765144  0.57891156]]\n",
      "\n",
      "  [[-0.05170068  0.5829608 ]\n",
      "   [-0.05251053  0.58377065]\n",
      "   [-0.05332038  0.5845805 ]\n",
      "   [-0.05413022  0.58539035]]\n",
      "\n",
      "  [[-0.05817946  0.58943959]\n",
      "   [-0.05898931  0.59024943]\n",
      "   [-0.05979916  0.59105928]\n",
      "   [-0.06060901  0.59186913]]]] [[[[-0.03874312  0.57000324]\n",
      "   [-0.03955296  0.57081309]\n",
      "   [-0.04036281  0.57162293]\n",
      "   [-0.04117266  0.57243278]]\n",
      "\n",
      "  [[-0.0452219   0.57648202]\n",
      "   [-0.04603175  0.57729187]\n",
      "   [-0.04684159  0.57810172]\n",
      "   [-0.04765144  0.57891156]]\n",
      "\n",
      "  [[-0.05170068  0.5829608 ]\n",
      "   [-0.05251053  0.58377065]\n",
      "   [-0.05332038  0.5845805 ]\n",
      "   [-0.05413022  0.58539035]]\n",
      "\n",
      "  [[-0.05817946  0.58943959]\n",
      "   [-0.05898931  0.59024943]\n",
      "   [-0.05979916  0.59105928]\n",
      "   [-0.06060901  0.59186913]]]]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the convolutional forward function\n",
    "input_image = np.linspace(-0.1, 0.4, num=1*8*8*1).reshape([1, 8, 8, 1]) # a single 8 by 8 grayscale image\n",
    "in_channels, k_size, n_filt = 1, 5, 2\n",
    "\n",
    "weight_size = k_size*k_size*in_channels*n_filt\n",
    "bias_size = n_filt\n",
    "\n",
    "\n",
    "\n",
    "single_conv = ConvLayer2D(in_channels, k_size, n_filt, stride=1, padding=0, name=\"conv_test\")\n",
    "\n",
    "w = np.linspace(-0.2, 0.2, num=weight_size).reshape(k_size, k_size, in_channels, n_filt)\n",
    "b = np.linspace(-0.3, 0.3, num=bias_size)\n",
    "\n",
    "single_conv.params[single_conv.w_name] = w\n",
    "single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "out = single_conv.forward(input_image)\n",
    "\n",
    "print(\"Received output shape: {}, Expected output shape: (1, 4, 4, 2)\".format(out.shape))\n",
    "\n",
    "correct_out = np.array([[\n",
    "   [[-0.03874312, 0.57000324],\n",
    "   [-0.03955296, 0.57081309],\n",
    "   [-0.04036281, 0.57162293],\n",
    "   [-0.04117266, 0.57243278]],\n",
    "\n",
    "  [[-0.0452219, 0.57648202],\n",
    "   [-0.04603175, 0.57729187],\n",
    "   [-0.04684159, 0.57810172],\n",
    "   [-0.04765144, 0.57891156]],\n",
    "\n",
    "  [[-0.05170068, 0.5829608 ],\n",
    "   [-0.05251053, 0.58377065],\n",
    "   [-0.05332038, 0.5845805 ],\n",
    "   [-0.05413022, 0.58539035]],\n",
    "\n",
    "  [[-0.05817946, 0.58943959],\n",
    "   [-0.05898931, 0.59024943],\n",
    "   [-0.05979916, 0.59105928],\n",
    "   [-0.06060901, 0.59186913]]]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))\n",
    "print(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REMOVE LATER\n",
    "# img = np.random.randn(15, 8, 8, 3)\n",
    "# w = np.random.randn(4, 4, 3, 12)\n",
    "# b = np.random.randn(12)\n",
    "\n",
    "# single_conv = ConvLayer2D(input_channels=3, kernel_size=4, number_filters=12, stride=2, padding=1, name=\"conv_test\")\n",
    "# single_conv.params[single_conv.w_name] = w\n",
    "# single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "# C, N, K, P, S = 3, 12, 4, 1, 2\n",
    "# print('C : %s, N : %s, K : %s, P : %s, S : %s '%(C, N, K, P, S))\n",
    "\n",
    "# batch_size, input_height, input_width = img.shape[0],img.shape[1], img.shape[2]\n",
    "# output_height = ((input_height-K+(2*P))//S) + 1\n",
    "# output_width = ((input_width-K+(2*P))//S) + 1\n",
    "# print('in_height : %s, in_width : %s, out_height : %s, out_width : %s '%(input_height, input_width, output_height, output_width))\n",
    "\n",
    "# print('Input Image shape : ', img.shape)\n",
    "\n",
    "# img_padded = np.pad(img,((0, 0), (P, P), (P, P), (0, 0)), mode='constant')\n",
    "# print('Padded Image shape : ', img_padded.shape)\n",
    "\n",
    "# batch_size = img.shape[0]\n",
    "# conv_out = np.zeros((batch_size, output_height , output_width, N),dtype=img.dtype)\n",
    "# print('Conv out shape : ', conv_out.shape)\n",
    "\n",
    "\n",
    "# print(\"Weight : %s, bias : %s \"%(w.shape,b.shape))\n",
    "\n",
    "\n",
    "# # print(img_padded,w)\n",
    "# for batch in range(batch_size):\n",
    "#     x = img[batch]\n",
    "#     for jj in range(0,input_width-K+1,S):\n",
    "#         for ii in range(0,input_height-K+1,S):\n",
    "#             for f in range(N):\n",
    "#                 x = img_padded[batch, ii:ii + K, jj : jj+K, :]\n",
    "#                 print(x.shape)\n",
    "#                 x_re = x.reshape(-1,1)\n",
    "#                 print('X RE********* : \\n',x_re.shape)\n",
    "#                 wt = w[:,:,:,f] # [K*K*C,f]\n",
    "#                 print(wt.shape)\n",
    "#                 wt_re = wt.reshape(-1,1)\n",
    "#                 bias = b[f]\n",
    "#                 print('WT RE********* : \\n',wt_re.shape, bias.shape)\n",
    "#                 o = np.dot(wt_re.T,x_re)+bias\n",
    "#                 print('O ********* : \\n',o.shape)\n",
    "#                 print(batch,ii,jj,f)\n",
    "#                 conv_out[batch, ii//S, jj//S, f] = o\n",
    "                \n",
    "# #               print('CONET ********* : \\n',conv_out)\n",
    "                        \n",
    "\n",
    "# print('Post feature extraction output : ',conv_out.shape, conv_out)        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Layer Backward [5pts]\n",
    "\n",
    "Now complete the backward pass of a convolutional layer. Fill in the TODO block in the `backward` function of the ConvLayer2D class. Check you results with this code and expect differences of less than 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimg Error:  1.0\n",
      "dw Error:  6.086693619917458e-09\n",
      "db Error:  7.162524851651021e-11\n",
      "dimg Shape:  (8, 8, 3) (15, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the conv backward function\n",
    "img = np.random.randn(15, 8, 8, 3)\n",
    "w = np.random.randn(4, 4, 3, 12)\n",
    "b = np.random.randn(12)\n",
    "dout = np.random.randn(15, 4, 4, 12)\n",
    "\n",
    "single_conv = ConvLayer2D(input_channels=3, kernel_size=4, number_filters=12, stride=2, padding=1, name=\"conv_test\")\n",
    "single_conv.params[single_conv.w_name] = w\n",
    "single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "dimg_num = eval_numerical_gradient_array(lambda x: single_conv.forward(img), img, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: single_conv.forward(img), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: single_conv.forward(img), b, dout)\n",
    "\n",
    "out = single_conv.forward(img)\n",
    "\n",
    "dimg = single_conv.backward(dout)\n",
    "dw = single_conv.grads[single_conv.w_name]\n",
    "db = single_conv.grads[single_conv.b_name]\n",
    "\n",
    "# The error should be around 1e-6\n",
    "print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# The errors should be around 1e-8\n",
    "print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "print(\"db Error: \", rel_error(db_num, db))\n",
    "# # The shapes should be same\n",
    "print(\"dimg Shape: \", dimg.shape, img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4, 4, 12) (15, 8, 8, 3) (4, 4, 3, 12)\n",
      "(16, 3, 1)\n",
      "[[[ 13.80623783  -9.07677695   4.66811354]\n",
      "  [ -6.48612888 -13.56808237  -4.4724802 ]\n",
      "  [ 12.42091269  -3.11829129 -22.73072552]\n",
      "  [  0.78410859  -4.29688929   8.19188727]]\n",
      "\n",
      " [[ 12.72617243   0.1762492  -16.26687134]\n",
      "  [  2.2714259    1.01632358  -3.16925659]\n",
      "  [-10.65104935 -24.35057325 -32.22459641]\n",
      "  [ -5.34512411  19.11527033  14.84854585]]\n",
      "\n",
      " [[-17.64865627 -30.531795     5.47147199]\n",
      "  [ -7.36376667   5.24737776 -12.53552094]\n",
      "  [-18.78381317  -9.46164513  25.44017159]\n",
      "  [ 11.14247541 -16.29407979  11.16963442]]\n",
      "\n",
      " [[ 15.89988919  -9.17711254   6.63548808]\n",
      "  [  6.28740807 -14.45740032   3.15264423]\n",
      "  [-24.24530606  -1.50202496  10.73562752]\n",
      "  [ 15.61518754  -8.75539177  -2.41690558]]] \n",
      "************\n",
      " [[[ 13.80623783  -9.07677695   4.66811354]\n",
      "  [ -6.48612888 -13.56808237  -4.4724802 ]\n",
      "  [ 12.42091269  -3.11829129 -22.73072552]\n",
      "  [  0.78410859  -4.29688929   8.19188727]]\n",
      "\n",
      " [[ 12.72617243   0.1762492  -16.26687134]\n",
      "  [  2.2714259    1.01632357  -3.16925659]\n",
      "  [-10.65104935 -24.35057325 -32.22459641]\n",
      "  [ -5.34512411  19.11527033  14.84854585]]\n",
      "\n",
      " [[-17.64865627 -30.531795     5.47147199]\n",
      "  [ -7.36376667   5.24737776 -12.53552094]\n",
      "  [-18.78381317  -9.46164513  25.44017159]\n",
      "  [ 11.14247541 -16.29407979  11.16963442]]\n",
      "\n",
      " [[ 15.89988919  -9.17711254   6.63548808]\n",
      "  [  6.28740807 -14.45740032   3.15264423]\n",
      "  [-24.24530606  -1.50202496  10.73562752]\n",
      "  [ 15.61518754  -8.75539177  -2.41690558]]]\n",
      "[ -6.83922291 -17.32681194  19.98709623 -16.27372979  -4.76437336\n",
      "  -8.63125837 -17.32756453 -10.0693098  -26.42483491   4.76946783\n",
      " -23.68556661 -18.14894275] \n",
      "************\n",
      " [ -6.83922291 -17.32681194  19.98709623 -16.27372979  -4.76437336\n",
      "  -8.63125837 -17.32756453 -10.0693098  -26.42483491   4.76946783\n",
      " -23.68556661 -18.14894275]\n"
     ]
    }
   ],
   "source": [
    "print(dout.shape, img.shape, w.shape)\n",
    "print(w[:,:,:,0].reshape(-1,w.shape[2],1).shape)\n",
    "print(dw[:,:,:,0],'\\n************\\n',dw_num[:,:,:,0])\n",
    "print(db,'\\n************\\n',db_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "\n",
    "# # Test the conv backward function\n",
    "# img = np.random.randn(15, 8, 8, 3)\n",
    "# w = np.random.randn(4, 4, 3, 12)\n",
    "# b = np.random.randn(12)\n",
    "# dout = np.random.randn(15, 4, 4, 12)\n",
    "\n",
    "# single_conv = ConvLayer2D(input_channels=3, kernel_size=4, number_filters=12, stride=2, padding=1, name=\"conv_test\")\n",
    "# single_conv.params[single_conv.w_name] = w\n",
    "# single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "# dimg_num = eval_numerical_gradient_array(lambda x: single_conv.forward(img), img, dout)\n",
    "# dw_num = eval_numerical_gradient_array(lambda w: single_conv.forward(img), w, dout)\n",
    "# db_num = eval_numerical_gradient_array(lambda b: single_conv.forward(img), b, dout)\n",
    "\n",
    "# out = single_conv.forward(img)\n",
    "\n",
    "# dimg = single_conv.backward(dout)\n",
    "# dw = single_conv.grads[single_conv.w_name]\n",
    "# db = single_conv.grads[single_conv.b_name]\n",
    "\n",
    "# # The error should be around 1e-6\n",
    "# print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# # The errors should be around 1e-8\n",
    "# print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "# print(\"db Error: \", rel_error(db_num, db))\n",
    "# # # The shapes should be same\n",
    "# print(\"dimg Shape: \", dimg.shape, img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE LATER\n",
    "# for i in range(m):\n",
    "#     for ii in range(0,input_height + (2*P)-K+1,S):\n",
    "#         for jj in range(0,input_width+ (2*P)-K+1,S):\n",
    "#             for c in range(C):\n",
    "#                 for f in range(N):\n",
    "#                     dL_dW = dprev[batch,:,:,f].reshape(-1,1) \n",
    "#                     x = img_padded[batch, ii:ii + K, jj : jj+K, c].reshape(-1,1)\n",
    "#                     dw[ii//S, jj//S,c, f] = np.dot(dL_dW.T,x)\n",
    "\n",
    "#                     dL_dX = dprev[batch,:,:,f].reshape(-1,1) \n",
    "#                     w_re = w[:,:, c, f].reshape(-1,1)\n",
    "#                     dx_padded[batch, ii//S, jj//S, c] += np.dot(dL_dX.T,w_re)\n",
    "\n",
    "# dx_depadded = dx_padded[:, P:-P,P:-P, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without paddinng or stride\n",
    "# for batch in range(batch_size):\n",
    "#     for ii in range(0,input_height-K):\n",
    "#         for jj in range(0,input_width-K):\n",
    "#             for f in range(N):\n",
    "#                 dL_dO = dprev[batch,:,:,f].reshape(-1,1) # [K*K*C,f]\n",
    "#                 for c in range(C):\n",
    "#                     x = img[batch, ii:ii + K, jj : jj+K, c].reshape(-1,1)\n",
    "# #                     print(img[batch, ii:ii + K, jj : jj+K, c],'\\n\\n', dprev[batch,:,:,f])\n",
    "# #                     print(x,'\\n\\n', dL_dO)\n",
    "#                     dw[ii, jj,c, f] = np.dot(dL_dO.T,x)\n",
    "# #                     print(dw[ii, jj,c, f],dw_num[ii,jj,c,f])\n",
    "#                     # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "# #                     da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "# #                     dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "# #                     db[:,:,:,c] += dZ[i, h, w, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling Layer\n",
    "Now we will implement maxpooling layers, which can help to reduce the image size while preserving the overall structure of the image.\n",
    "\n",
    "### Forward Pass max pooling [5pts]\n",
    "Fill out the TODO block in the `forward` function of the MaxPoolingLayer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received output shape: (1, 3, 3, 1), Expected output shape: (1, 3, 3, 1)\n",
      "Difference:  1.8750000280978013e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the convolutional forward function\n",
    "input_image = np.linspace(-0.1, 0.4, num=64).reshape([1, 8, 8, 1]) # a single 8 by 8 grayscale image\n",
    "\n",
    "maxpool= MaxPoolingLayer(pool_size=4, stride=2, name=\"maxpool_test\")\n",
    "out = maxpool.forward(input_image)\n",
    "\n",
    "print(\"Received output shape: {}, Expected output shape: (1, 3, 3, 1)\".format(out.shape))\n",
    "\n",
    "correct_out = np.array([[\n",
    "   [[0.11428571],\n",
    "   [0.13015873],\n",
    "   [0.14603175]],\n",
    "\n",
    "  [[0.24126984],\n",
    "   [0.25714286],\n",
    "   [0.27301587]],\n",
    "\n",
    "  [[0.36825397],\n",
    "   [0.38412698],\n",
    "   [0.4       ]]]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.11428571]\n",
      "   [0.13015873]\n",
      "   [0.14603175]]\n",
      "\n",
      "  [[0.24126984]\n",
      "   [0.25714286]\n",
      "   [0.27301587]]\n",
      "\n",
      "  [[0.36825397]\n",
      "   [0.38412698]\n",
      "   [0.4       ]]]] \n",
      "****\n",
      " [[[[0.11428571]\n",
      "   [0.13015873]\n",
      "   [0.14603175]]\n",
      "\n",
      "  [[0.24126984]\n",
      "   [0.25714286]\n",
      "   [0.27301587]]\n",
      "\n",
      "  [[0.36825397]\n",
      "   [0.38412698]\n",
      "   [0.4       ]]]]\n"
     ]
    }
   ],
   "source": [
    "#REMOVE LATER\n",
    "print(out,'\\n****\\n', correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass Max pooling [5pts]\n",
    "Fill out the `backward` function in the MaxPoolingLayer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimg Error:  3.275924472097173e-12\n",
      "dimg Shape:  (15, 8, 8, 3) (15, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "img = np.random.randn(15, 8, 8, 3)\n",
    "\n",
    "dout = np.random.randn(15, 3, 3, 3)\n",
    "\n",
    "maxpool= MaxPoolingLayer(pool_size=4, stride=2, name=\"maxpool_test\")\n",
    "\n",
    "dimg_num = eval_numerical_gradient_array(lambda x: maxpool.forward(img), img, dout)\n",
    "\n",
    "out = maxpool.forward(img)\n",
    "dimg = maxpool.backward(dout)\n",
    "\n",
    "# The error should be around 1e-8\n",
    "print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# The shapes should be same\n",
    "print(\"dimg Shape: \", dimg.shape, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3, 3, 3)\n",
      "[[[ 0.          0.          0.95658979  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.31150733  0.        ]\n",
      "  [ 0.          2.61180818  0.          0.          0.\n",
      "    0.12661216  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    1.66546818  0.          0.        ]\n",
      "  [ 0.          0.64037704  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         -3.16129821  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.90857355  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "   -0.34013628  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          2.09924178  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          3.01759625  0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.         -0.05439309\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.91581433  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -1.96260553  0.\n",
      "   -2.95814731  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.         -1.39210141  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.90913396  0.        ]\n",
      "  [ 0.          0.5811172   0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -1.32521815  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.31293967]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -1.10014443\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.         -1.16299117  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.08904738\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.32515179  0.          0.\n",
      "    0.          0.          2.10183366]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          1.75860079  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.30103403]\n",
      "  [ 0.          0.33875438  1.51170574  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -2.85197091  0.          0.\n",
      "    0.         -2.5768571   0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -1.25185975]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          1.25997816\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.00478795]\n",
      "  [-2.14013973  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.67653543  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.61367142\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.29564238]\n",
      "  [ 0.          0.         -0.70714853  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.0088395 ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.63559663]\n",
      "  [ 0.          0.          0.00662706  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.14458316]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -2.6512676   0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.16054255  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.41908775  0.          2.07357621\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.2491307   0.          0.          0.          0.\n",
      "    0.          0.          1.83633527]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -1.42928849  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.69417647\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.7867799   0.        ]\n",
      "  [ 0.          0.         -0.05298732  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.42755106  0.          0.          0.          3.23877745\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.93765582  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -1.43750129]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.         -1.10250656  0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.14896431  0.        ]\n",
      "  [ 0.          0.         -2.95206773  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          2.00228111  0.\n",
      "   -3.70170833  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -1.53978174]\n",
      "  [ 0.          0.          0.         -1.23175037  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    1.64172614  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.260764    0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         -2.19847391  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.69738695  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [-2.78592542  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.96285511\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.56627442\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "   -0.28209735  0.          0.        ]\n",
      "  [ 0.          0.          0.07366742  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.1337741   0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 1.55596808  0.          0.          0.          0.\n",
      "   -0.55449624  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.5745925   0.\n",
      "    0.          0.         -1.08771078]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]] \n",
      "****\n",
      " [[[ 0.          0.          0.95658979  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.31150733  0.        ]\n",
      "  [ 0.          2.61180818  0.          0.          0.\n",
      "    0.12661216  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    1.66546818  0.          0.        ]\n",
      "  [ 0.          0.64037704  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         -3.16129821  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.90857355  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "   -0.34013628  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          2.09924178  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          3.01759625  0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.         -0.05439309\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.91581433  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -1.96260553  0.\n",
      "   -2.95814731  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.         -1.39210141  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.90913396  0.        ]\n",
      "  [ 0.          0.5811172   0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -1.32521815  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.31293967]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -1.10014443\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.         -1.16299117  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.08904738\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.32515179  0.          0.\n",
      "    0.          0.          2.10183366]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          1.75860079  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.30103403]\n",
      "  [ 0.          0.33875438  1.51170574  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -2.85197091  0.          0.\n",
      "    0.         -2.5768571   0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -1.25185975]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          1.25997816\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.00478795]\n",
      "  [-2.14013973  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.67653543  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.61367142\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.29564238]\n",
      "  [ 0.          0.         -0.70714853  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.0088395 ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.63559663]\n",
      "  [ 0.          0.          0.00662706  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.14458316]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -2.6512676   0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.16054255  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.41908775  0.          2.07357621\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.2491307   0.          0.          0.          0.\n",
      "    0.          0.          1.83633527]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -1.42928849  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.69417647\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.7867799   0.        ]\n",
      "  [ 0.          0.         -0.05298732  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.42755106  0.          0.          0.          3.23877745\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.93765582  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -1.43750129]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.         -1.10250656  0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.14896431  0.        ]\n",
      "  [ 0.          0.         -2.95206773  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          2.00228111  0.\n",
      "   -3.70170833  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -1.53978174]\n",
      "  [ 0.          0.          0.         -1.23175037  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    1.64172614  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.260764    0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         -2.19847391  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.69738695  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [-2.78592542  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.96285511\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.56627442\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "   -0.28209735  0.          0.        ]\n",
      "  [ 0.          0.          0.07366742  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.1337741   0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 1.55596808  0.          0.          0.          0.\n",
      "   -0.55449624  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.5745925   0.\n",
      "    0.          0.         -1.08771078]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#REMOVE LATER\n",
    "print(out.shape)\n",
    "print(dimg_num[:,:,:,0],'\\n****\\n', dimg[:,:,:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Small Convolutional Neural Network [3pts]\n",
    "Please find the `TestCNN` class in `lib/cnn/cnn_models.py`.\n",
    "Again you only need to complete few lines of code in the TODO block.\n",
    "Please design a Convolutional --> Maxpool --> flatten --> fc network where the shapes of parameters match the given shapes.\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively.\n",
    "Here you only modify the param_name part, the _w, and _b are automatically assigned during network setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = TestCNN()\n",
    "loss_func = cross_entropy()\n",
    "\n",
    "B, H, W, iC = 4, 8, 8, 3 #batch, height, width, in_channels\n",
    "k = 3 #kernel size\n",
    "oC, Hi, O = 3, 27, 5 # out channels, Hidden Layer input, Output size\n",
    "std = 0.02\n",
    "x = np.random.randn(B,H,W,iC)\n",
    "y = np.random.randint(O, size=B)\n",
    "\n",
    "print (\"Testing initialization ... \")\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "w1_std = abs(model.net.get_params(\"conv1_w\").std() - std)\n",
    "b1 = model.net.get_params(\"conv1_b\").std()\n",
    "w2_std = abs(model.net.get_params(\"fc1_w\").std() - std)\n",
    "b2 = model.net.get_params(\"fc1_b\").std()\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "assert w1_std < std / 10, \"First layer weights do not seem right\"\n",
    "assert np.all(b1 == 0), \"First layer biases do not seem right\"\n",
    "assert w2_std < std / 10, \"Second layer weights do not seem right\"\n",
    "assert np.all(b2 == 0), \"Second layer biases do not seem right\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing test-time forward pass ... \")\n",
    "w1 = np.linspace(-0.7, 0.3, num=k*k*iC*oC).reshape(k,k,iC,oC)\n",
    "w2 = np.linspace(-0.2, 0.2, num=Hi*O).reshape(Hi, O)\n",
    "b1 = np.linspace(-0.6, 0.2, num=oC)\n",
    "b2 = np.linspace(-0.9, 0.1, num=O)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "model.net.assign(\"conv1_w\", w1)\n",
    "model.net.assign(\"conv1_b\", b1)\n",
    "model.net.assign(\"fc1_w\", w2)\n",
    "model.net.assign(\"fc1_b\", b2)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "feats = np.linspace(-5.5, 4.5, num=B*H*W*iC).reshape(B,H,W,iC)\n",
    "scores = model.forward(feats)\n",
    "correct_scores = np.asarray([[-13.85107294, -11.52845818,  -9.20584342,  -6.88322866,  -4.5606139 ],\n",
    " [-11.44514171, -10.21200524 , -8.97886878 , -7.74573231 , -6.51259584],\n",
    " [ -9.03921048,  -8.89555231 , -8.75189413 , -8.60823596,  -8.46457778],\n",
    " [ -6.63327925 , -7.57909937 , -8.52491949 , -9.4707396 , -10.41655972]])\n",
    "scores_diff = np.sum(np.abs(scores - correct_scores))\n",
    "assert scores_diff < 1e-6, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the loss ...\",)\n",
    "y = np.asarray([0, 2, 1, 4])\n",
    "loss = loss_func.forward(scores, y)\n",
    "dLoss = loss_func.backward()\n",
    "correct_loss = 4.56046848799693\n",
    "assert abs(loss - correct_loss) < 1e-10, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the gradients (error should be no larger than 1e-6) ...\")\n",
    "din = model.backward(dLoss)\n",
    "for layer in model.net.layers:\n",
    "    if not layer.params:\n",
    "        continue\n",
    "    for name in sorted(layer.grads):\n",
    "        f = lambda _: loss_func.forward(model.forward(feats), y)\n",
    "        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)\n",
    "        print ('%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network [25pts]\n",
    "In this section, we defined a `SmallConvolutionalNetwork` class for you to fill in the TODO block in `lib/cnn/cnn_models.py`.\n",
    "\n",
    "Here please design a network with at most two convolutions and two maxpooling layers (you may use less).\n",
    "You can adjust the parameters for any layer, and include layers other than those listed above that you have implemented (such as fully-connected layers and non-linearities).\n",
    "You are also free to select any optimizer you have implemented (with any learning rate).\n",
    "\n",
    "You will train your network on CIFAR-100 20-way superclass classification.\n",
    "Try to find a combination that is able to achieve 40% validation accuracy.\n",
    "\n",
    "Since the CNN takes significantly longer to train than the fully connected network, it is suggested to start off with fewer filters in your Conv layers and fewer intermediate fully-connected layers so as to get faster initial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the data\n",
    "data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"], data[\"labels_train\"]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (40000, 32, 32, 3)\n",
      "Flattened data input size: 3072\n",
      "Number of data classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data_dict[\"data_train\"][0].shape)\n",
    "print(\"Flattened data input size:\", np.prod(data[\"data_train\"].shape[1:]))\n",
    "print(\"Number of data classes:\", max(data['labels_train']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = SmallConvolutionalNetwork()\n",
    "loss_f = cross_entropy()\n",
    "\n",
    "\n",
    "results = None\n",
    "#############################################################################\n",
    "# TODO: Use the train_net function you completed to train a network         #\n",
    "# You may only adjust the hyperparameters within this block                 #\n",
    "#############################################################################\n",
    "optimizer = Adam(model.net, 1e-3)\n",
    "\n",
    "batch_size = 1000\n",
    "epochs = 5\n",
    "lr_decay = .999\n",
    "lr_decay_every = 10\n",
    "regularization = \"none\"\n",
    "reg_lambda = 0.01\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "results = train_net(data_dict, model, loss_f, optimizer, batch_size, epochs, \n",
    "                    lr_decay, lr_decay_every, show_every=4000, verbose=True, regularization=regularization, reg_lambda=reg_lambda)\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_cnn_train.txt', 'wb') as f:\n",
    "    pickle.dump(results_cnn_train,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to generate the training plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100]  # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.plot(val_acc_hist, '-o', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Layers [5pts]\n",
    "\n",
    "An interesting finding from early research in convolutional networks was that the learned convolutions resembled filters used for things like edge detection. Complete the code below to visualize the filters in the first convolutional layer of your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_array = None\n",
    "nrows, ncols = None, None\n",
    "\n",
    "###################################################\n",
    "# TODO: read the weights in the convolutional     #\n",
    "# layer and reshape them to a grid of images to   #\n",
    "# view with matplotlib.                           #\n",
    "###################################################\n",
    "\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "plt.imshow(im_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline Question: Comment below on what kinds of filters you see. Include your response in your submission [5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Credit: Analysis on Trained Model [5pts]\n",
    "\n",
    "For extra credit, you can perform some additional analysis of your trained model. Some suggested analyses are:\n",
    "1. Plot the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) of your model's predictions on the test set. Look for trends to see which classes are frequently misclassified as other classes (e.g. are the two vehicle superclasses frequently confused with each other?).\n",
    "2. Implement [BatchNorm](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739) and analyze how the models train with and without BatchNorm.\n",
    "3. Introduce some small noise in the labels, and investigate how that affects training and validation accuracy.\n",
    "\n",
    "You are free to choose any analysis question of interest to you. We will not be providing any starter code for the extra credit. Include your extra-credit analysis as the final section of your report pdf, titled \"Extra Credit\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Please prepare a PDF document `problem_2_solution.pdf` in the root directory of this repository with all plots and inline answers of your solution. Concretely, the document should contain the following items in strict order:\n",
    "1. Training loss / accuracy curves for CNN training\n",
    "2. Visualization of convolutional filters\n",
    "3. Answers to inline questions about convolutional filters\n",
    "\n",
    "Note that you still need to submit the jupyter notebook with all generated solutions. We will randomly pick submissions and check that the plots in the PDF and in the notebook are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbconvert import HTMLExporter\n",
    "import codecs\n",
    "import nbformat\n",
    "\n",
    "notebook_name = 'Problem_2.ipynb'\n",
    "output_file_name = 'output_prob_2.html'\n",
    "\n",
    "exporter = HTMLExporter()\n",
    "output_notebook = nbformat.read(notebook_name, as_version=4)\n",
    "\n",
    "output, resources = exporter.from_notebook_node(output_notebook)\n",
    "codecs.open(output_file_name, 'w', encoding='utf-8').write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
